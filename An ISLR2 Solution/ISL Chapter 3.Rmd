---
title: "Chapter 3 Linear Regression"
output:
  pdf_document: default
---

```{r}
library(MASS)
library(ISLR2)
```

# Exercise 1

Statistically, TV and radio are significant predictors but newspaper. This means newspaper has no effect (no association) with Sales. We can reject the null hypothesis coefficients of TV and radio being 0 and we fail to reject the null hypothesis coefficient of newspaper being 0.

# Exercise 2

KNN classifier outputs discrete values (labels) whereas KNN regression outputs continuous values.

KNN classifier makes decisions based on the conditional probability for a specific class from K training observations. KNN regression, however, averages the K nearest training observations.

# Exercise 3

Starting salary after graduation = 50 + 20  \* GPA + 0.07 \* IQ + 35 \* Level + 0.01 \* GPA \* IQ - 10 \* GPA \* Level (1)

## a

Since IQ and GPA are fixed, we can let them be 100 and 3. Plug these numbers into (1), we have:

Salary = 120 + 35 \* Level - 10 \* GPA \* Level = 120 + Level \* (35 - 10 \* GPA)

High school graduate salary - College graduate salary = 0 - 1 \* (35 - 10 \* GPA) = 10 \* GPA - 35

Therefore, we cannot say whether statement i, ii is true or false. 

For statement iii, if GPA is high enough (greater than 3.5), it is true.

For statement iv, if GPA is high enough (greater than 3.5), it is false.

## b

Predicted salary = 50 + 20 \* 4.0 + 0.07 \* 110 + 35 \* 1  + 0.01 \* 4 \* 110 - 10 \* 4 \* 1 = \$ 137.1K

## c

False since the coefficient does not tell us how large the effect is. (It is the p-value of the coefficient).

# Exercise 4

## a

We would expect the cubic regression to have a lower training RSS than the linear regression because it could make a tighter fit

## b

The cubic regression is likely to overfit the test data so we expect that the RSS of linear regression to be lower.

## c

We expect that the RSS of linear regression to be lower.

## d

It depends on how far the true relationship between X and Y is from linear. The further the it is from linear, the lower the RSS of the cubic regression (relative to the RSS of the linear regression).

# Exercise 5

If we just plug $$ \widehat{\beta} $$ into the fitted value equation, we get:

$$
\widehat{y}_{i}=x_{i}\dfrac{\sum ^{n}_{i=1}x_{i}y_{i}}{\sum ^{n}_{i'=1}x_{i'}^2}
$$

However, this could be problematic due to the "i" inside and outside the sum symbol. Instead, we can rewrite:

$$
\widehat{y_{i}} = x_{i} \widehat{\beta}
$$

and,

$$
\widehat{\beta}=\dfrac{\sum ^{n}_{i=1}x_{i'}y_{i'}}{\sum ^{n}_{j=1}x_{j}^2}
$$

Plugging $$ \widehat{\beta} $$ again into the fitted value equation:

$$
\widehat{y}_{i}=x_{i}\dfrac{\sum ^{n}_{i'=1}x_{i'}y_{i'}}{\sum ^{n}_{j=1}x_{j}^2}
$$

$$
\Leftrightarrow 
\sum^{n}_{i'=1}\dfrac{x_{i'}x_{i}}{\sum ^{n}_{j=1}x_{j}}y_{i'}
$$

$$
\Leftrightarrow 
\sum^{n}_{i'=1}a_{i'}y_{i'}
$$

with $$ a_{i'} = \dfrac{x_{i'}x_{i}}{\sum ^{n}_{j=1}x_{j}} $$

# Exercise 6

From (3.2), we have the least square line equation:

$$
\widehat{y} = \widehat{\beta_{0}} + \widehat{\beta_{1}}x
$$

And from (3.4), we have:

$$
\widehat{\beta_{0}} = \overline{y} - \widehat{\beta_{1}} \overline{x}
$$

Plug $$ \widehat{\beta_{0}} $$ into (3.2):

$$
\widehat{y} =  \widehat{\beta_{0}} + \widehat{\beta_{1}} \overline{x}
$$

$$
\Leftrightarrow \widehat{y} =  \widehat{\beta_{0}} + \overline{y} - \widehat{\beta_{0}} = \overline{y}
$$

Hence, in the case of simple linear regression, the least squares line always passes through the point ( $$ \overline{x}, \overline{y} $$ )

# Exercise 7

On the one hand,

$$
R^2 =\frac{\sum_{i=1}^n\left(\hat{y}_i-\bar{y}\right)^2}{\sum_{i=1}^n\left(y_i-\bar{y}\right)^2}
$$

$$
=\frac{\sum_{i=1}^n\left(\hat{\beta}_0+\hat{\beta}_1 x_i-\bar{y}\right)^2}{\sum_{i=1}^n\left(y_i-\bar{y}\right)^2}
$$

$$
=\frac{\sum_{i=1}^n\left(\bar{y}-\hat{\beta}_1 \bar{x}+\hat{\beta}_1 x_i-\bar{y}\right)^2}{\sum_{i=1}^n\left(y_i-\bar{y}\right)^2}
$$

$$
=\frac{\sum_{i=1}^n\left(\hat{\beta}_1\left(x_i-\bar{x}\right)\right)^2}{\sum_{i=1}^n\left(y_i-\bar{y}\right)^2}
$$

$$
=\hat{\beta}_1^2 \frac{\frac{1}{n-1} \sum_{i=1}^n\left(x_i-\bar{x}\right)^2}{\frac{1}{n-1} \sum_{i=1}^n\left(y_i-\bar{y}\right)^2}
$$

$$
=\hat{\beta}_1^2 \frac{\sigma_x^2}{\sigma_y^2} 
$$

On the other hand,

$$
Cor(X, Y) = \dfrac{Cov(X, Y)}{\sigma_x \sigma_{y}}
$$

$$
\rho_{xy} = \dfrac{\sigma_{xy}}{\sigma_x \sigma_{y}}
$$

Also,

$$
\hat{\beta}_1=\frac{\sum_{i=1}^n\left(x_i-\bar{x}\right)\left(y_i-\bar{y}\right)}{\sum_{i=1}^n\left(x_i-\bar{x}\right)^2}
$$

$$
\hat{\beta}_1=\frac{\dfrac{1}{n-1}\sum_{i=1}^n\left(x_i-\bar{x}\right)\left(y_i-\bar{y}\right)}{\dfrac{1}{n-1}\sum_{i=1}^n\left(x_i-\bar{x}\right)^2}
$$

$$
\hat{\beta}_1=\frac{\sigma_{xy}}{\sigma_{x}^2}
$$

Therefore, we can write the correlation as

$$
\rho_{xy} = \dfrac{\hat{\beta}_1 \sigma_x }{\sigma_{y}}
$$

$$
\rho_{xy}^2=\hat{\beta}_1^2 \frac{\sigma_x^2}{\sigma_y^2} = R^2
$$

# Exercise 8

## a

```{r}
head(Auto)
```

The result suggests that there is a relationship between horsepower and mpg. The relationship is moderately strong and negative.

```{r}
attach(Auto)
eighta_model = lm(mpg ~ horsepower, data = Auto)
summary(eighta_model)
```

```{r}
newdata = data.frame(horsepower = 98)
predict(eighta_model, newdata = newdata, interval = 'confidence')
```

```{r}
predict(eighta_model, newdata = newdata, interval = 'prediction')
```

## b

```{r}
plot(horsepower, mpg)
abline(eighta_model, col = 2)
```

## c

```{r}
plot(eighta_model)
```

# Exercise 9

## a

```{r}
library(psych)
pairs.panels(Auto)
```

```{r}
colnames(Auto)
```

## b

```{r}
data.frame(cor(Auto[, 1:8]))
```

## c

Predictors displacement, weight, year and origin appear to have statistically significant relationship to the response.

```{r}
ninec_model = lm(mpg ~ . - name, data = Auto)
summary(ninec_model)
```

## d

Observations 323, 326 and 327 are large outliers. The plot also indentifies the 14th observation as a high leverage.

```{r}
plot(ninec_model)
```

## e

Using the output from (c), we can choose displacement, weight and year to fit a model with interaction effects. Displacement \* weight appears to be statistically significant as a result.

```{r}
nined_model = lm(mpg ~ displacement * weight + displacement * year +
                   year * weight, data = Auto[, 1:8])
summary(nined_model)
```

## f

In general, the more features are added to the model, the higher the performance.

```{r}
ninef_model_one = lm(mpg ~ . + log(horsepower) - name - horsepower, data = Auto)
summary(ninef_model_one)
```

```{r}
ninef_model_two = lm(mpg ~ . + log(horsepower) + log(weight) + log(displacement)- 
                       name - horsepower, data = Auto)
summary(ninef_model_two)
```

```{r}
ninef_model_three = lm(mpg ~ . + poly(horsepower, 4) - name, data = Auto)
summary(ninef_model_three)
```

# Exercise 10

## a

```{r}
head(Carseats)
```

```{r}
ex_ten_model = lm(Sales ~ Price + Urban + US, data = Carseats)
```

## b

If the Price increases by 1 (unit), Sales decreases by 0.05 on average, given the other predictors remain unchanged.

If the person is from urban area, the Sales decreases by 0.05 on average, given the other predictors remain unchanged.

If the person is from the US, the Sales decreases by 0.05 on average, given the other predictors remain unchanged.

## c

Sales = 13.04 - 0.05 \* Price - 0.02 \* UrbanYes + 1.2 \* USYes

```{r}
summary(ex_ten_model)
```

## d

We can reject the null hypothesis H0: coefficients $$ \beta $$ = 0 for predictors Price and US

## e

```{r}
smaller_model = lm(Sales ~ Price + US, data = Carseats)
```

## f

Using RSE (Residual Standard Error) and R-squared as metrics, we can see that the smaller model performs better although this is not significant. Both models explain 23% - 24% the variability in Sales. And the lack of fit is about 2.4 - 2.5

```{r}
summary(ex_ten_model)
summary(smaller_model)
```

## g

```{r}
smaller_model_summary = data.frame(summary(smaller_model)$coefficients)
smaller_model_summary$Estimate - 2 * smaller_model_summary$Std..Error
```

```{r}
confint(smaller_model)
```

## h

Observation 43 appears to be the high leverage.

```{r}
plot(smaller_model)
```

```{r}
prices = Carseats$Price
mean_price = mean(prices)
numerator = (prices - mean_price)^2
denominator = sum(numerator)
n = length(prices)

high_leverage_stat = 1/n + numerator / denominator
high_leverage_stat[1:10]
```

```{r}
hatvalues(lm(Sales ~ Price, data = Carseats))[1:10]
```

```{r}
h_stat = hatvalues(smaller_model)
plot(h_stat)
```

```{r}
which(h_stat > 0.04)
```

# Exercise 11

```{r}
set.seed(1)
x = rnorm(100)
y = 2 * x + rnorm(100)
plot(x, y)
```

## a

```{r}
no_intercept_y_onto_x = lm(y ~ x + 0)
summary(no_intercept_y_onto_x)
# summary(lm(y ~ x))
```

## b

```{r}
no_intercept_x_onto_y = lm(x ~ y + 0)
summary(no_intercept_x_onto_y)
# summary(lm(x ~ y))
```

## c

The t-statistic from both models are the same, hence, p-values are equal also.

From the first model, we can write y = 2x + $$ \epsilon $$

From the first model, we can write x = 0.5x + $$ \epsilon $$

## d

$$
t = \dfrac{\widehat{\beta} - \beta_0}{SE(\widehat{\beta})}
$$

$$
t=\left. \dfrac{\sum ^{n}_{i=1}x_{i}y_{i}}{\sum ^{n}_{i'=1}x_{i'}^{2}} \middle/ \sqrt{\dfrac{\sum ^{n}_{i=1}\left( y-x_{i}\widehat{\beta }\right) ^{2}}{\left( n-1\right) \sum ^{n}_{i'=1}x_{i'}^{2}}} \right.
$$

$$
=
\dfrac{\sqrt{n-1}\sum ^{n}_{i=1}x_{i}y_{i}} {\sqrt{\sum ^{n}_{i'=1}x_{i'}^{2}}\sqrt{\sum ^{n}_{i=1}\left( y_{i} -x_{i}\widehat{\beta}\right) ^{2}}}
$$

$$
=
\dfrac{\sqrt{n-1}\sum ^{n}_{i=1}x_{i}y_{i}} {\sqrt{
\sum ^{n}_{i'=1}x_{i'}^{2} \left( \sum ^{n}_{i=1}y_i^{2}-\sum ^{n}_{i=1}2y_{i}x_{i}\widehat{\beta} + \sum ^{n}_{i=1}x_i^{2}\widehat{\beta}^2 \right)
}}
$$

Just keep the numerator as that way and work out the denominator,

$$
denominator^2=
\sum ^{n}_{i'=1}x_{i'}^{2}\sum ^{n}_{i=1}y_i^{2}-2\sum ^{n}_{i'=1}x_{i'}^{2}\sum ^{n}_{i=1}y_{i}x_{i}\widehat{\beta} + 
\sum ^{n}_{i'=1}x_{i'}^{2} \sum ^{n}_{i=1}x_i^{2}\widehat{\beta}^2
$$

$$
=
\sum ^{n}_{i'=1}x_{i'}^{2}\sum ^{n}_{i=1}y_i^{2}
-2\sum ^{n}_{i'=1}x_{i'}^{2}\sum ^{n}_{i=1}y_i x_{i}\dfrac{\sum ^{n}_{j=1}x_{j}y_{j}}{\sum ^{n}_{k=1}x_{k}^{2}} +
\sum ^{n}_{i'=1}x_{i'}^{2}\sum ^{n}_{i=1}x_{i}^{2} \left( \dfrac{\sum ^{n}_{j=1}x_{j}y_j}{\sum ^{n}_{k=1}x_{k}^{2}} \right)^2
$$

Note that these terms are the same:

$$
\sum ^{n}_{i'=1}x_{i'}^{2} = \sum ^{n}_{k=1}x_{k}^{2} = \sum ^{n}_{i=1}x_{i}^{2}
$$

And,

$$
\sum ^{n}_{j=1}x_{j}y_{j} = \sum ^{n}_{i=1}x_{i}y_{i}
$$

Hence,

$$
= \sum ^{n}_{i'=1}x_{i'}^{2}\sum ^{n}_{i=1}y_i^{2}
- \left( \sum ^{n}_{i=1}x_{i}y_{i} \right) ^2
$$

$$
= \sum ^{n}_{i=1}x_{i}^{2}\sum ^{n}_{i'=1}y_{i'}^{2}
- \left( \sum ^{n}_{i'=1}x_{i'}y_{i'} \right) ^2
$$

Finally, the t-statistic can be written as:

$$
\dfrac{\sqrt{n-1}\sum ^{n}_{i=1}x_{i}y_{i}} {\sqrt{
\sum ^{n}_{i=1}x_{i}^{2}\sum ^{n}_{i'=1}y_{i'}^{2}
- \left( \sum ^{n}_{i'=1}x_{i'}y_{i'} \right) ^2
}}
$$

## e

Obviously, if there is only one variable and one response, the role of the variable and the response in the t-statistic are interchangeable.

$$
\dfrac{\sqrt{n-1}\sum ^{n}_{i=1}x_{i}y_{i}} {\sqrt{
\sum ^{n}_{i=1}x_{i}^{2}\sum ^{n}_{i'=1}y_{i'}^{2}
- \left( \sum ^{n}_{i'=1}x_{i'}y_{i'} \right) ^2
}} 
= \dfrac{\sqrt{n-1}\sum ^{n}_{i=1}y_{i}x_{i}} {\sqrt{
\sum ^{n}_{i=1}y_{i}^{2}\sum ^{n}_{i'=1}x_{i'}^{2}
- \left( \sum ^{n}_{i'=1}y_{i'}x_{i'} \right) ^2
}}
$$

## f

```{r}
summary(lm(y ~ x))
summary(lm(x ~ y))
```

# Exercise 12

## a

The coefficient estimate for the regression of X onto Y will be the same as the coefficient for the regression from Y onto X when sum squared of $$ x_i $$ equals to sum squared of $$ y_i $$

## b

```{r}
set.seed(1)
xx = rnorm(100)
yy = 2 * xx
```

```{r}
summary(lm(yy ~ xx + 0))
summary(lm(xx ~ yy + 0))
```

## c

```{r}
set.seed(1)
xxx = rnorm(100)
yyy = sample(xxx)

summary(lm(yyy ~ xxx + 0))
summary(lm(xxx ~ yyy + 0))
```

# Exercise 13

## a

```{r}
set.seed(1)
x = rnorm(100, 0, 1)
```

## b

```{r}
set.seed(1)
eps = rnorm(100, 0, 0.25)
```

## c

y length is 100, $$ \beta_{0} $$ = -1, $$ \beta_{1} $$ = 0.5.

x and y is linearly related. The relationship is strong and positive.

```{r}
y = -1 + 0.5*x + eps
```

## d

```{r}
plot(x, y)
```

## e

The obtained coefficients (beta hat) are close to the actual coefficients.

```{r}
first_model = lm(y ~ x)
first_model$coefficients
```

## f

```{r}
plot(x, y)
abline(first_model, col = 'red', lty = 1)
abline(a = -1, b = 0.5, col = 'blue', lty = 2)
legend(x = 'topleft',legend = c('Prediction Line', 'Population Line'), 
       col = c('red', 'blue'), lty = c(1, 2))
```

## g

Using RSE and R-squared, the polynomial regression model slightly improved the model fit.

```{r}
second_model = lm(y ~ poly(x, 2))
summary(first_model)
summary(second_model)
```

```{r}
plot(x, y)
points(x, second_model$fitted.values, col = 'red')
```

## h

```{r}
set.seed(1)

x = rnorm(100, 0, 1)
eps2 = rnorm(100, 0, 0.09)
y2 = -1 + 0.5*x + eps2

plot(x, y2)
```

```{r}
third_model = lm(y2 ~ x)
fourth_model = lm(y2 ~ poly(x, 2))

plot(x, y)
points(x, third_model$fitted.values, col = 'red')
points(x, fourth_model$fitted.values, col = 'blue')
```

## i

```{r}
set.seed(1)

x = rnorm(100, 0, 1)
eps3 = rnorm(100, 0, 0.64)
y3 = -1 + 0.5*x + eps3

fifth_model = lm(y3 ~ x)
sixth_model = lm(y3 ~ x + I(x^2))

plot(x, y3)
points(x, fifth_model$fitted.values, col = 'red')
points(x, sixth_model$fitted.values, col = 'blue')
```

## j

Obviously, the model is more confident when there is less noise and vice versa.

```{r}
confint(first_model)
confint(third_model)
confint(fifth_model)
```

```{r}
summary(lm(y ~ x + I(x^2)))
summary(lm(y ~ poly(x, 2)))
```

# Exercise 14

## a

$$ Y = \beta_0 + \beta_1 * X_1 + \beta_2 * X_2 $$

```{r}
set.seed(1)
x1 = runif(100)
x2 = 0.5 * x1 + rnorm(100)/10
y = 2 + 2*x1 + 0.3*x2 + rnorm(100)
```

## b

Variables x1 and x2 linearly correlated.

```{r}
plot(x1, x2)
```

## c

$$ \widehat{\beta_0}, \widehat{\beta_1}, \widehat{\beta_2} $$ are 2.1, 1.4 and 1 respectively. Recall that the "true" $$ \beta_0, \beta_1, \beta_2 $$ are 2, 2 and 0.3 respectively.

In this case, only $$ \widehat{\beta_0} $$ is close to its "true" value. Also, we can reject to the null hypothesis that $$ \beta_1 = 0 $$, but we fail to reject to the null hypothesis that $$ \beta_2 = 0 $$.

```{r}
first_model = lm(y ~ x1 + x2)
summary(first_model)
```

## d

$$ \widehat{\beta_0}, \widehat{\beta_1} $$ are 2.1, 2.0 respectively. Recall that the "true" $$ \beta_0, \beta_1, \beta_2 $$ are 2, 2 and 0.3 respectively.

In this case, $$ \widehat{\beta_0}, \widehat{\beta_1} $$ are close to its true values. Also, we can reject to the null hypothesis that $$ \beta_1 $$ = 0.

RSE and Adjusted R-squared are almost the same as in the first model.

```{r}
second_model = lm(y ~ x1)
summary(second_model)
```

## e

In this case, we can reject to the null hypothesis that $$ \beta_2 $$ = 0.

```{r}
third_model = lm(y ~ x2)
summary(third_model)
```

## f

The results from (c) - (e) contradict each other but this makes sense since collinearity is in presence. The response can be predicted using x1 or x2 only.

```{r}
summary(first_model)$coefficients
summary(second_model)$coefficients
summary(third_model)$coefficients
```

## g

```{r}
set.seed(1)

x1 = runif(100)
x2 = 0.5 * x1 + rnorm(100)/10
y = 2 + 2*x1 + 0.3*x2 + rnorm(100)

x1 = c(x1, 0.1)
x2 = c(x2, 0.8)
y = c(y, 6)
```

In this case (y \~ x1 + x2), the additional observation is a high-leverage point.

```{r}
fourth_model = lm(y ~ x1 + x2)
summary(fourth_model)
plot(fourth_model)
```

In this case (y \~ x1), the additional observation is an outlier.

```{r}
fifth_model = lm(y ~ x1)
plot(fifth_model)
```

In this case (y \~ x2), the additional observation is a high-leverage point.

```{r}
sixth_model = lm(y ~ x2)
plot(sixth_model)
```

# Exercise 15

```{r}
head(Boston)
```

## a

Except "chas", all other predictors appear to be statistically significant.

```{r}
all_predictors = colnames(Boston)
individual_predictor_result = data.frame()

for (i in 2:13){
  
  x = Boston[, i]
  model = lm(crim ~ x, data = Boston)
  df = round(data.frame(summary(model)$coefficients), 3)[2,]
  individual_predictor_result = rbind(individual_predictor_result, df)
  individual_predictor_result = individual_predictor_result
}

rownames(individual_predictor_result) = all_predictors[2:13]
individual_predictor_result
# View(individual_predictor_result)
```

## b

We reject the null hypothesis H0: $$ \beta $$ = 0 for predictors zn, dis, rad and medv

```{r}
full_model = lm(crim ~ ., data = Boston)
all_predictors_result = round(data.frame(summary(full_model)$coefficients), 3)
all_predictors_result
# View(all_predictors_result)
```

## c

```{r}
individual_predictor_coefficients = data.frame()

for (i in 1:13){
  
  x = Boston[, i]
  model = lm(crim ~ x, data = Boston)
  df = round(data.frame(model$coefficients), 3)[2,]
  individual_predictor_coefficients = rbind(individual_predictor_coefficients, df)
  individual_predictor_coefficients = individual_predictor_coefficients
}

individual_predictor_coefficients = individual_predictor_coefficients[2:13,]
```

```{r}
all_predictors_coefficients = full_model$coefficients
all_predictors_coefficients = data.frame(all_predictors_coefficients)[2:13, 1]
all_predictors_coefficients
```

```{r}
plot(individual_predictor_coefficients, all_predictors_coefficients)
```

## d

```{r}
library(psych)
pairs.panels(Boston)
```

There is evidence of non-linear association between all of these predictors and the response.

```{r}
summary(lm(crim ~ poly(zn, 3), data = Boston))
summary(lm(crim ~ poly(indus, 3), data = Boston))
summary(lm(crim ~ poly(nox, 3), data = Boston))
summary(lm(crim ~ poly(rm, 3), data = Boston))
summary(lm(crim ~ poly(age, 3), data = Boston))
summary(lm(crim ~ poly(dis, 3), data = Boston))
summary(lm(crim ~ poly(rad, 3), data = Boston))
summary(lm(crim ~ poly(tax, 3), data = Boston))
summary(lm(crim ~ poly(ptratio, 3), data = Boston))
summary(lm(crim ~ poly(lstat, 3), data = Boston))
summary(lm(crim ~ poly(medv, 3), data = Boston))
```
