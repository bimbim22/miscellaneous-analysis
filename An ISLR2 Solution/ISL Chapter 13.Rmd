---
title: "Chapter 13 Multiple Testing"
output:
  pdf_document: default
---

# Exercise 1

## a

We expect to make $$ \alpha \times m$$ type I errors.

## b

The FWER is $$ 1 - (1 - \alpha)^m $$

## c

When m = 2, the FWER in b is

$$
1 - (1 - \alpha)^2
$$

which is in general relatively small.

If the 2 p-values are positive correlated, when one is small and smaller than $$ \alpha $$

then the other one tends to be small and also smaller than $$\alpha$$. The probability that we at least make one type I error raises (compared to the FWER in b) .

On the other hand, when one is large and larger $$ \alpha $$

then the other one tends to be large and larger than $$\alpha$$ as well. The probability that we at least make one type I error declines (compared to the FWER in b) .

## d

In general, if the 2 p-values are negatively correlated, when one is small and smaller than $$ \alpha $$

then the other one tends to be large and larger than $$\alpha$$. The probability that we at least make one type I error raises (compared to the FWER in b).

# Exercise 2

## a

The distribution of $$A_{j}$$ is Bernoulli distribution.

## b 

The distribution of $$\sum_{j=1}^{m} A_{j}$$ is Binomial distribution (sum of Bernoulli distribution). Each reject is equivalent to one success.

## c

The standard deviation is

$$
\sigma = \sqrt{m \alpha (1 - \alpha)}
$$

# Exercise 3

We want to prove

$$
1 - \prod_{j=1}^{m} (1 - \alpha_{j}) \le  \sum_{j=1}^{m} {\alpha_{j}} \hspace{20pt} (1)
$$

Assuming (1) is true, we can easily get:

$$
(1-\alpha_{1})(1-\alpha_{2})...(1-\alpha_{m}) \ge 1 - (\alpha_{1} +  \alpha_{2} + ... + \alpha_{m}) $$
$$ \Leftrightarrow (1-\alpha_{1})(1-\alpha_{2})...(1-\alpha_{m})(1-\alpha_{m+1}) \ge (1-\alpha_{m+1})(1 - (\alpha_{1} +  \alpha_{2} + ... + \alpha_{m})) $$
$$\Leftrightarrow (1-\alpha_{1})(1-\alpha_{2})...(1-\alpha_{m})(1-\alpha_{m+1}) \ge (1-\alpha_{m+1})(1 - (\alpha_{1} +  \alpha_{2} + ... + \alpha_{m} + \alpha_{m+1}) + \alpha_{m+1}) $$
$$\Leftrightarrow
\prod_{j=1}^{m+1} (1 - \alpha_{j}) \ge (1 - \alpha_{m+1})(1 - \sum_{j=1}^{m+1} {\alpha_{j}} + \alpha_{m+1}) \hspace{20pt} (2)
$$

On the other hand, we have:

$$
\sum_{j=1}^{m} \alpha_{j} \ge 0 $$
$$\Leftrightarrow
-1 + \sum_{j=1}^{m+1} {\alpha_{j}} + 1 - \alpha_{m+1}  \ge0 $$
$$\Leftrightarrow
-(1 - \sum_{j=1}^{m+1} {\alpha_{j}}) + 1 - \alpha_{m+1}  \ge0 $$
$$\Leftrightarrow
-(1 - \sum_{j=1}^{m+1} {\alpha_{j}})\alpha_{m+1} + (1 - \alpha_{m+1})\alpha_{m+1}  \ge0 $$
$$\Leftrightarrow
(1 - \sum_{j=1}^{m+1} {\alpha_{j}})-(1 - \sum_{j=1}^{m+1} {\alpha_{j}})\alpha_{m+1} + (1 - \alpha_{m+1})\alpha_{m+1}  \ge  (1 - \sum_{j=1}^{m+1} {\alpha_{j}}) $$
$$\Leftrightarrow (1 - \alpha_{m+1})(1 - \sum_{j=1}^{m+1} {\alpha_{j}} + \alpha_{m+1}) \ge  (1 - \sum_{j=1}^{m+1} {\alpha_{j}})  \hspace{20pt} (3)
$$

From (2) and (3), we can write:

$$
\prod_{j=1}^{m+1} (1 - \alpha_{j}) \ge  1 - \sum_{j=1}^{m+1} {\alpha_{j}} $$
$$\Leftrightarrow
1 - \prod_{j=1}^{m+1} (1 - \alpha_{j}) \le  \sum_{j=1}^{m+1} {\alpha_{j}}
$$

By assuming (1) is true, we can prove that it's also true with $$ m + 1 $$ .

In other words, the family-wise error rate is no greater than $$ \sum_{j=1}^{m+1} {\alpha_{j}} $$

# Exercise 4

## a

We reject these six hypotheses $$  H_{01}, H_{02}, H_{03}, H_{08}, H_{09}, H_{10} $$.

## b

To control FWER at level 0.05 while testing m = 4 null hypotheses, we should reject null hypotheses whose p-values are less than $$ 0.05/4 = 0.0125 $$. And they are hypotheses $$  H_{01}, H_{08}, H_{09}, H_{10} $$.

## c

```{r}
null_hypothesis = c('H01', 'H02', 'H03', 'H04', 'H05', 
                    'H06', 'H07', 'H08', 'H09', 'H10')
p_values = c(0.0011, 0.31, 0.017, 0.32, 0.11,
             0.90, 0.07, 0.006, 0.004, 0.0009)
data = data.frame(null_hypothesis, p_values)
data
```

```{r}
ordered_data = data[order(p_values), ]
q = 0.05
p_j = matrix()
m = dim(data)[1]
for (i in 1:10){
  ordered_data$p_j[i] = q * i / m
  ordered_data$reject[i] = ordered_data$p_values[i] < ordered_data$p_j[i]
}
```

As a result, we reject five null hypotheses $$ H_{01}, H_{03}, H_{08}, H_{09}, H_{10} $$.

```{r}
ordered_data
```

## d

As a result, we reject seven null hypotheses $$ H_{01}, H_{03}, H_{05}, H_{07}, H_{08}, H_{09}, H_{10} $$.

```{r}
ordered_data = data[order(p_values), ]
q = 0.2
p_j = matrix()
m = dim(data)[1]
for (i in 1:10){
  ordered_data$p_j[i] = q * i / m
  ordered_data$reject[i] = ordered_data$p_values[i] < ordered_data$p_j[i]
}

ordered_data
```

## e

We have rejected seven null hypotheses from (d) and there are approximately $$ 0.2 \times 7 \approx 1.4 $$ (from 1 to 2) false positives.

# Exercise 5

## a

```{r}
null_hypothesis = c('H01', 'H02', 'H03', 'H04', 'H05')
p_values = c(0.0011, 0.1, 0.2, 0.025, 0.034)
data = data.frame(null_hypothesis, p_values)

ordered_data = data[order(p_values), ]
alpha = 0.1
p_j = matrix()
m = dim(data)[1]

for (i in 1:m){
  
  ordered_data$Bonferroni[i] = alpha / m
  ordered_data$Bonferroni_reject[i] = ordered_data$p_values[i] < ordered_data$Bonferroni[i] 
  
  ordered_data$Holm[i] = alpha / (m + 1 - i)
  ordered_data$Holm_reject[i] = ordered_data$p_values[i] < ordered_data$Holm[i]
  
}

ordered_data
```

## b

```{r}
null_hypothesis = c('H01', 'H02', 'H03', 'H04', 'H05')
p_values = c(0.0011, 0.1, 0.2, 0.021, 0.034)
data = data.frame(null_hypothesis, p_values)

ordered_data = data[order(p_values), ]
alpha = 0.1
p_j = matrix()
m = dim(data)[1]

for (i in 1:m){
  
  ordered_data$Bonferroni[i] = alpha / m
  ordered_data$Bonferroni_reject[i] = ordered_data$p_values[i] < ordered_data$Bonferroni[i] 
  
  ordered_data$Holm[i] = alpha / (m + 1 - i)
  ordered_data$Holm_reject[i] = ordered_data$p_values[i] < ordered_data$Holm[i]
  
}

ordered_data
```

# Exercise 6

## a

|                 |         |         |         |
|-----------------|---------|---------|---------|
|                 | Panel 1 | Panel 2 | Panel 3 |
| False positives | 0       | 0       | 0       |
| False negatives | 1       | 1       | 5       |
| True positives  | 7       | 7       | 3       |
| True negatives  | 2       | 2       | 2       |
| Type I errors   | 0       | 0       | 0       |
| Type II errors  | 1       | 1       | 5       |

: Bonferroni procedure

## b

|                 | Panel 1 | Panel 2 | Panel 3 |
|-----------------|---------|---------|---------|
| False positives | 0       | 0       | 0       |
| False negatives | 1       | 0       | 0       |
| True positives  | 7       | 8       | 8       |
| True negatives  | 2       | 2       | 2       |
| Type I errors   | 0       | 0       | 0       |
| Type II errors  | 1       | 0       | 0       |

: Holm procedure

## c

The false discovery rate associated with using the Bonferroni procedure to control the FWER at level $$\alpha$$ = 0.05 is 0.

## d

The false discovery rate associated with using the Holm procedure to control the FWER at level $$\alpha$$ = 0.05 is 0.

## e

|                 | Panel 1 | Panel 2 | Panel 3 |
|-----------------|---------|---------|---------|
| False positives | 0       | 0       | 0       |
| False negatives | 5       | 7       | 6       |
| True positives  | 3       | 1       | 2       |
| True negatives  | 2       | 2       | 2       |
| Type I errors   | 0       | 0       | 0       |
| Type II errors  | 5       | 7       | 6       |

: Bonferroni procedure

There is no change in the FDR, it's still 0.

# Exercise 7

```{r}
library(ISLR2)
head(Carseats)
```

## a

```{r}
model1 = lm(Sales ~ CompPrice, data = Carseats)
summary(model1)
```

```{r}
model2 = lm(Sales ~ Income, data = Carseats)
summary(model2)
```

```{r}
model3 = lm(Sales ~ Advertising, data = Carseats)
summary(model3)
```

```{r}
model4 = lm(Sales ~ Population, data = Carseats)
summary(model4)
```

```{r}
model5 = lm(Sales ~ Price, data = Carseats)
summary(model5)
```

```{r}
model6 = lm(Sales ~ Age, data = Carseats)
summary(model6)
```

```{r}
model7 = lm(Sales ~ Education, data = Carseats)
summary(model7)
```

```{r}
variables = c('CompPrice', 'Income', 'Advertising', 'Population',
              'Price', 'Age', 'Education')
p_values = c(0.201, 0.00231, 4.38e-08, 0.314, 2e-16, 2.79e-06, 0.3)
report = data.frame(variables, p_values)
report
```

## b

```{r}
m = dim(report)[1]
alpha = 0.05
for (i in 1:m){
  report$reject[i] = report$p_values[i] < alpha
}
report
```

## c

```{r}
FWER = 0.05
for (i in 1:m){
  report$FWER_reject[i] = report$p_values[i] < FWER/m
}
report
```

## d

```{r}
ordered_report = report[order(p_values), ]
q = 0.2
for (i in 1:m){
  p_j = q / m * i
  ordered_report$FDR_reject[i] = ordered_report$p_values[i] < p_j
}
ordered_report
```

# Exercise 8

```{r}
set.seed(1)
n = 20
m = 100
X = matrix(rnorm(n * m), ncol = m)
```

## a

```{r}
p_values = matrix()
for (i in 1:m){
  p_values[i] = t.test(X[, i], mu = 0)$p.value
}
hist(p_values)
```

## b

We would reject 4 null hypothesis. This is roughly equal to 100 \* 0.05 = 5

```{r}
sum(p_values < 0.05)
```

## c

If we control FWER at level 0.05, we would reject 0 null hypotheses.

```{r}
FWER = 0.05
m = 100
sum(p_values < FWER/m)
```

## d

If we control FDR at level 0.05, we would reject 0 null hypotheses.

```{r}
ordered_pvalues = data.frame(p_values[order(p_values)])
colnames(ordered_pvalues) = 'p_values'
q = 0.05
for (i in 1:m){
  p_j = q / m * i
  ordered_pvalues$FDR_reject[i] = ordered_pvalues$p_values[i] < p_j
}
sum(ordered_pvalues$FDR_reject)
```

## e

If we control the FWER for just these 10 cherry-picked managers, we would reject all 1 hypothesis.

```{r}
ave_returns = apply(X, 2, mean)
cherry_picked_returns = sort(ave_returns, decreasing = T)[1:10]
cherry_picked_indices = match(cherry_picked_returns, ave_returns)
```

```{r}
m = 10

p_values = matrix()
for (i in 1:m){
  index = cherry_picked_indices[i]
  p_values[i] = t.test(X[, index], mu = 0)$p.value
}

FWER = 0.05
sum(p_values < FWER/m)
```

If we control the FDR for just these 10 cherry-picked managers, we would reject 1 hypothesis.

```{r}
ordered_pvalues = data.frame(p_values[order(p_values)])
colnames(ordered_pvalues) = 'p_values'
q = 0.05
for (i in 1:m){
  p_j = q / m * i
  ordered_pvalues$FDR_reject[i] = ordered_pvalues$p_values[i] < p_j
}
sum(ordered_pvalues$FDR_reject)
```

## f

By cherry-picking the best managers, we have accidentally choose managers whose p-values are the smallest and this violates the assumption that all tested null hypotheses are adjusted for multiplicity.
