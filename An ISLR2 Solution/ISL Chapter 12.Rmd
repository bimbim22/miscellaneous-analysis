---
title: "Chapter 12"
output:
  pdf_document: default
---

```{r}
options(repos="https://cran.rstudio.com" )
```

```{r}
library(ggplot2)
library(softImpute)
```

# Exercise 1

## a

$$
\frac{1}{\left|C_k\right|}  \sum_{i, i^{\prime} \in C_k} \sum_{j=1}^p\left(x_{i j}-x_{i^{\prime} j}\right)^2=\frac{1}{\left|C_k\right|} \sum_{i, i^{\prime} \in C_k} \sum_{j=1}^p\left(x_{i j}-\bar{x}_{k j}+\bar{x}_{k j}-x_{i^{\prime} j}\right)^2 
$$

$$
=\frac{1}{\left|C_k\right|} \sum_{i, i^{\prime} \in C_k} \sum_{j=1}^p\left(\left(x_{i j}-\bar{x}_{k j}\right)-\left(x_{i^{\prime} j}-\bar{x}_{k j}\right)\right)^2
$$

$$
=\frac{1}{\left|C_k\right|} \sum_{i, i^{\prime} \in C_k} \sum_{j=1}^p\left(\left(x_{i j}-\bar{x}_{k j}\right)^2-2\left(x_{i j}-\bar{x}_{k j}\right)\left(x_{i^{\prime} j}-\bar{x}_{k j}\right)+\left(x_{i^{\prime} j}-\bar{x}_{k j}\right)^2\right)
$$

$$
=\frac{1}{\left|C_k\right|} \sum_{i, i^{\prime} \in C_k} \sum_{j=1}^p\left(x_{i j}-\bar{x}_{k j}\right)^2-\frac{2}{\left|C_k\right|} \sum_{i, i^{\prime} \in C_k} \sum_{j=1}^p\left(x_{i j}-\bar{x}_{k j}\right)\left(x_{i^{\prime} j}-\bar{x}_{k j}\right)+\frac{1}{\left|C_k\right|} \sum_{i, i^{\prime} \in C_k} \sum_{j=1}^p\left(x_{i^{\prime} j}-\bar{x}_{k j}\right)^2
$$

$$
=\frac{\left|C_k\right|}{\left|C_k\right|} \sum_{i \in C_k} \sum_{j=1}^p\left(x_{i j}-\bar{x}_{k j}\right)^2-\frac{2}{\left|C_k\right|} \sum_{i, i^{\prime} \in C_k} \sum_{j=1}^p\left(x_{i j}-\bar{x}_{k j}\right)\left(x_{i^{\prime} j}-\bar{x}_{kj}\right)+\frac{\left|C_k\right|}{\left|C_k\right|} \sum_{i^{\prime} \in C_k} \sum_{j=1}^p\left(x_{i^{\prime} j}-\bar{x}_{k j}\right)^2
$$

$$
=\sum_{i \in C_k} \sum_{j=1}^p\left(x_{i j}-\bar{x}_{k j}\right)^2-\frac{2}{\left|C_k\right|} \sum_{i, i^{\prime} \in C_k} \sum_{j=1}^p\left(x_{i j}-\bar{x}_{k j}\right)\left(x_{i^{\prime} j}-\bar{x}_{k j}\right)+\sum_{i \in C_k} \sum_{j=1}^p\left(\bar{x}_{k j}-x_{i j}\right)^2
$$

$$
=2\sum_{i \in C_k} \sum_{j=1}^p\left(x_{i j}-\bar{x}_{k j}\right)^2-\frac{2}{\left|C_k\right|} \sum_{i, i^{\prime} \in C_k} \sum_{j=1}^p\left(x_{i j}-\bar{x}_{k j}\right)\left(x_{i^{\prime} j}-\bar{x}_{k j}\right)
$$

$$
=2 \sum_{i \in C_k} \sum_{j=1}^p\left(x_{i j}-\bar{x}_{k j}\right)^2-0  =2 \sum_{i \in C_k} \sum_{j=1}^p\left(x_{i j}-\bar{x}_{k j}\right)^2
$$

## b

The sum of squared distance of each observation from the cluster mean, which is the value of RHS, decreases after each iteration. Hence, the clustering algorithm decreases the objective at each iteration.

# Exercise 2

## a

```{r}
observations = as.dist(matrix(c(0,0.3,0.4,0.7,
                                0.3, 0, 0.5, 0.8,
                                0.4, 0.5, 0, 0.45,
                                0.7, 0.8, 0.45, 0), nrow = 4))

plot(hclust(observations, method="complete"))
```

## b

```{r}
plot(hclust(observations, method="single"))
```

## c

```{r}
plot(hclust(observations, method="complete"))
abline(h = 0.7, col = "red")
```

## d

```{r}
plot(hclust(observations, method="single"))
abline(h = 0.44, col = "red") 
```

## e

```{r}
plot(hclust(observations, method = 'complete'), labels = c(2, 1, 4, 3))
```

# Exercise 3

## a

```{r}
data = data.frame(X1 = c(1,1,0,5,6,4), X2 = c(4,3,4,1,2,0))
plot(data[,'X1'], data[,'X2'])
```

## b

```{r}
set.seed(0)
random_sample = sample(1:nrow(data), 3)

plot(data[,'X1'], data[,'X2'])

points(data[random_sample, 'X1'], data[random_sample, 'X2'], col = 'red')
points(data[-random_sample, 'X1'], data[-random_sample, 'X2'], col = 'green')
```

## c

```{r}
set.seed(0)
random_sample = sample(1:nrow(data), 3)

centroid_1 = c(sum(data[random_sample, 1])/3, sum(data[random_sample, 2])/3)
centroid_2 = c(sum(data[-random_sample, 1])/3, sum(data[-random_sample, 2])/3)

plot(data[,'X1'], data[,'X2'])
points(centroid_1[1], centroid_1[2], col="red", pch = 2)
points(centroid_2[1], centroid_2[2], col="green", pch = 2)

points(data[random_sample, 'X1'], data[random_sample, 'X2'], col = 'red')
points(data[-random_sample, 'X1'], data[-random_sample, 'X2'], col = 'green')
```

## d

```{r}
for (i in 1:6){
  euclidean_dist_1 = sqrt((data[i, 1] - centroid_1[1])^2 + (data[i, 2] -centroid_1[2])^2)
  euclidean_dist_2 = sqrt((data[i,1] - centroid_2[1])^2 + (data[i, 2] - centroid_2[2])^2)
  
  if (euclidean_dist_1 < euclidean_dist_2) {
    data$cluster[i] = 1
  }
  else {
    data$cluster[i] = 2
  }
}

data
```

```{r}
plot(data[,'X1'], data[,'X2'])
points(centroid_1[1], centroid_1[2], col="red", pch = 2)
points(centroid_2[1], centroid_2[2], col="green", pch = 2)

points(data[data$cluster == 1, 'X1'], data[data$cluster == 1, 'X2'], col = 'red')
points(data[data$cluster == 2, 'X1'], data[data$cluster == 2, 'X2'], col = 'green')
```

## e

```{r}
centroid_1 = c(sum(data[data$cluster == 1,1])/3,sum(data[data$cluster == 1,2])/3)
centroid_2 = c(sum(data[data$cluster == 2,1])/3,sum(data[data$cluster == 2,2])/3)

plot(data[,'X1'], data[,'X2'])
points(centroid_1[1], centroid_1[2], col="red", pch = 2)
points(centroid_2[1], centroid_2[2], col="green", pch = 2)

points(data[data$cluster == 1, 'X1'], data[data$cluster == 1, 'X2'], col = 'red')
points(data[data$cluster == 2, 'X1'], data[data$cluster == 2, 'X2'], col = 'green')
```

# Exercise 4

a/ There is not enough information to tell. It depends on the distances of of the observations.

b/ There is not enough information to tell. Although complete linkage records the maximal distance and single linkage records the minimal distance but these two distances could be the same and they fuse at the same height.

# Exercise 5

```{r}
socks = c(8, 11, 7, 6, 5, 6, 7, 8)
computers = c(0, 0, 0, 0, 1, 1, 1, 1)
shops = matrix(, nrow = 8)
for (i in 1:8){
  shops[i] = paste('shop', i)
}

eight_shoppers = data.frame(cbind(socks, computers))
row.names(eight_shoppers) = shops
eight_shoppers
plot(eight_shoppers)
```

## KMeans

```{r}
two_kmeans = kmeans(eight_shoppers, centers = 2)
plot(eight_shoppers, col = two_kmeans$cluster + 1, cex = 2, pch = 20)
```

```{r}
scaled_es = scale(eight_shoppers)
sclaed_two_km = kmeans(scaled_es, centers = 2)
plot(scaled_es, col = sclaed_two_km$cluster + 1, cex = 2, pch = 20)
```

## Hierarchical Clustering

```{r}
euclidean_dist = dist(scaled_es, method = 'euclidean')
eu_clusters = cutree(hclust(euclidean_dist), 2)
plot(eight_shoppers, col = eu_clusters + 1, cex = 2, pch = 20)
```

```{r}
library(factoextra)
cor_dist = get_dist(scaled_es, method = 'pearson')
cor_clusters = cutree(hclust(cor_dist), 2)
plot(eight_shoppers, col = cor_clusters + 1, cex = 2, pch = 20)
```

# Exercise 6

```{r}
head(USArrests)
```

```{r}
pca_usa = prcomp(scale(USArrests))
score_vectors = pca_usa$x
head(score_vectors)
```

```{r}
bind_data = data.frame(scale(USArrests), data.frame(score_vectors))
head(bind_data)
```

```{r}
coef(lm(Murder ~ PC1 + PC2 + PC3 + PC4, data = bind_data))
```

And these are equal to the loadings we want

```{r}
pca_usa = prcomp(scale(USArrests))
pca_usa$rotation
```

# Exercise 7

$$
d^2(r_{i},r_{j}) = 2(p-1)(1 - r_{ij})
$$

```{r}
scaled_usa = t(scale(t(USArrests)))
correlation = cor(t(scaled_usa))
squared_dist = as.matrix(dist(scaled_usa)**2)
n = dim(scaled_usa)[1]

dim(correlation)
dim(squared_dist)
n
```

```{r}
head(data.frame(squared_dist / (1 - correlation)))
```

# Exercise 8

## a

```{r}
scaled_usarrest = scale(USArrests)
pca_usarrest = prcomp(scaled_usarrest)
std_1 = pca_usarrest$sdev
var_1 = std_1**2
pve_1 = var_1 / sum(var_1)
pve_1
```

## b

```{r}
loadings = pca_usarrest$rotation
pve_2 = rep(NA, 4)
for (i in 1:4){
  scores = scaled_usarrest %*% loadings[, i]
  pve = sum(scores**2) / sum(scaled_usarrest**2)
  pve_2[i] = pve
}

pve_2
```

# Exercise 9

## a

```{r}
library(factoextra)
clusterability = get_clust_tendency(USArrests, n = 45)
clusterability$hopkins_stat
```

```{r}
hc_usarrest = hclust(dist(USArrests), method = 'complete')
```

## b

```{r}
plot(hc_usarrest, xlab = 'States', cex = 0.6, main = 'Complete Linkage')
data.frame(cutree(hc_usarrest, k = 3))
```

## c

```{r}
hc_scaled_usarrest = hclust(dist(scale(USArrests)), method = 'complete')
plot(hc_scaled_usarrest, xlab = 'States', cex = 0.6, main = 'Complete Linkage')
data.frame(cutree(hc_scaled_usarrest, 3))
```

## d

Scaling data before performing hierarchical clustering outputs a more balanced dendrogram and also avoid bias by some variables.

# Exercise 10

## a

```{r}
set.seed(42)
generated_data = matrix(rnorm(60*50), ncol = 50)
generated_data[1:20, ] = generated_data[1:20, ]
generated_data[21:40, ] = generated_data[21:40, ] - 2
generated_data[41:60, ] = generated_data[41:60, ] + 2
dim(generated_data)
```

```{r}
get_clust_tendency(generated_data, n = 50)
```

## b

```{r}
scaled_generated_data = scale(generated_data)
svd_generated_data = svd(generated_data)

M = 2
u = svd_generated_data$u[, 1:M]
d = svd_generated_data$d[1:M]
v = svd_generated_data$v[, 1:M]

reconstructed_data = u %*% (d * t(v))
get_clust_tendency(reconstructed_data, n = 50)$hopkins
plot(reconstructed_data, col = c(rep(1, 20), rep(2, 20), rep(3, 20)))
```

After being constructed, instances in each class became identical so we merely only see 3 dots on the plot.

## c

```{r}
three_kmeans = kmeans(generated_data, 3, nstart = 20)
three_kmeans$cluster
table(three_kmeans$cluster)
```

## d

```{r}
two_kmeans = kmeans(generated_data, 2, nstart = 20)
two_kmeans$cluster
table(two_kmeans$cluster)
```

There are 20 misclassifications in total, the algorithm class number 3 as 1.

## e

```{r}

four_kmeans = kmeans(generated_data, 4, nstart = 20)
four_kmeans$cluster
table(four_kmeans$cluster)
```

There are around 10 misclassifications in total, the algorithm split class number 3 as 2 classes. Since this is an unsupervised learning task, it's unfairly to judge the model to be correct or incorrect. In these cases, it's incorrect compared to the original data because we forced it to cluster that way.

## f

```{r}
three_kmeans = kmeans(reconstructed_data, 3, nstart = 20)
three_kmeans$cluster
get_clust_tendency(reconstructed_data, n = 50)$hopkins
table(three_kmeans$cluster)
```

```{r}
score_vectors = t(d %*% t(u))
variance = apply(score_vectors, 2, var)
variance / sum(variance)
```

All instances were clustered correctly. The first two principal components reserve almost 100% of total variance.

## g

```{r}
set.seed(42)
X1 = matrix(rnorm(60*50), ncol = 50)
X1[1:20, ] = X1[1:20, ]
X1[21:40, ] = X1[21:40, ] - 2
X1[41:60, ] = X1[41:60, ] + 2

kmeans_no_scaling = kmeans(X1, 3, nstart = 20)
kmeans_no_scaling$cluster
table(kmeans_no_scaling$cluster)

kmeans_scaling = kmeans(scale(X1), 3, nstart = 20)
kmeans_scaling$cluster
table(kmeans_scaling$cluster)
```

There is no significant difference between scaling and non-scaling before performing Kmeans in this particular case. (In practice, it really depends on the data characteristics to scale or not to scale or not to scale before clustering).

# Exercise 11

## First look

```{r}
library(MASS)
boston = Boston
head(boston)
```

```{r}
library(psych)
pairs.panels(boston)
table(boston$zn)
table(boston$chas)
hist(boston$black)
```

## Writing the function

```{r}
scaled_boston = scale(boston)

fill_nan = function(nan_rate, M){
  
  set.seed(42)
  row_nan_index = sample(nrow(boston), nan_rate * nrow(boston))
  col_nan_index = sample(ncol(boston), nan_rate * nrow(boston), replace = T)
  nan_index = cbind(row_nan_index, col_nan_index)
  Xna = scaled_boston
  Xna[nan_index] = NA

  imputation = softImpute(Xna, rank.max = M, maxit = 100, trace.it = F)
  imputed_Xhat = complete(Xna, imputation)
  correlation = cor(imputed_Xhat[nan_index], scaled_boston[nan_index])
  
  return(correlation)
  
}
```

```{r}
nan_rates = seq(0.05, 0.3, by = 0.05)
M_range = c(1:8)
correlations = matrix(nrow = length(M_range), ncol = length(nan_rates))

for (m in 1:length(M_range)){
  for (n in 1:length(nan_rates)){
    correlations[m, n] = fill_nan(nan_rates[n], M_range[m])
  }
}

correlations
```

```{r}
results = data.frame(correlations)
colnames(results) = nan_rates
rownames(results) = M_range
results
```

# Exercise 12

```{r}
set.seed(15)

X = data.matrix(scale(USArrests))

nomit = 20
ina = sample(seq(50), nomit)
inb = sample(4, nomit, replace = T)
Xna = X
nan_index = cbind(ina, inb)
Xna[nan_index] = NA
#View(Xna)
```

```{r}
xbar = colMeans(Xna, na.rm = T)
Xhat = Xna
Xhat[nan_index] = xbar[inb]
pca_Xhat = prcomp(Xhat)
```

```{r}
M = 1
pcs = pca_Xhat$rotation
reconstructed_usarrest = pca_Xhat$x[, 1:M] %*% t(pcs[, 1:M])
Xhat[nan_index] = reconstructed_usarrest[nan_index]

cor(Xhat[nan_index], X[nan_index])
```

```{r}
Xhat[1:5, ]
X[1:5, ]
```

# Exercise 13

## a

```{r}
gene_expression = read.csv('Ch12Ex13.csv', header = F)
dim(gene_expression)
gene_exp = gene_expression
head(data.frame(t(gene_exp)))
```

## b

```{r}
corr_dist = as.dist(1 - cor(gene_exp))
complete_hc = hclust(corr_dist, method = 'complete')
plot(complete_hc)
table(cutree(complete_hc, k = 2))
```

```{r}
average_hc = hclust(corr_dist, method = 'average')
plot(average_hc)
table(cutree(average_hc, k = 2))
```

```{r}
single_hc = hclust(corr_dist, method = 'single')
plot(single_hc)
table(cutree(single_hc, k = 2))
```

```{r}
centroid_hc = hclust(corr_dist, method = 'centroid')
plot(centroid_hc)
table(cutree(centroid_hc, k = 2))
```

The results are different depending on the type of linkage used.

## c

We can transpose the data set (use genes as observations and patients as predictors), perform pca on the transposed data set, then project observations on a 2-dimension space and see which genes differ the most.

```{r}
pca_gene = prcomp(scale(gene_exp))
loadings = pca_gene$rotation
two_dim_gene = as.matrix(gene_exp) %*% as.matrix(loadings[, 1:2])
```

```{r}
plot(two_dim_gene)
```

And these are genes that differ the most.

```{r}
which(two_dim_gene[, 1] < -5)
```
