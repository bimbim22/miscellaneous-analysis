---
title: "Chapter 6 Linear Model Selection and Regularization"
output:
  pdf_document: default
---

# Exercise 1

## a

The model with all k = predictors obtained from the best subset selection approach is the one whose training RSS the smallest.

## b

The model from best subset selection might has the smallest test RSS since it examines the most possible models.

## c

### i

True. At each step in the Forward stepwise selection, the previous predictor(s) are hold and then a new predictor is added.

### ii

True. At each step in the Backward stepwise selection, after one predictor is removed, the remaining predictors are hold.

### iii

False. There is no relationship between the predictors in the k-variable model identified by backward stepwise and the predictors in the (k + 1)-variable model identified by forward stepwise selection.

### iv

False. There is no relationship between the predictors in the k-variable model identified by forward stepwise and the predictors in the (k+1)-variable model identified by backward stepwise selection.

### v

False. There is no evidence nor enough information for this statement.

# Exercise 2

## a

The lasso, relative to least squares, is less flexible and hence will give improved prediction accuracy when its increase in bias is less than its decrease in variance. (Figure 6.8)

## b

The ridge regression, relative to least squares, is less flexible and hence will give improved prediction accuracy when its increase in bias is less than its decrease in variance. (Figure 6.5)

## c

The non-linear methods relative to least squares. more flexible and hence will give improved prediction accuracy when its increase in variance is less than its decrease in bias.

# Exercise 3

We can make use of figure 6.5 and 6.8 for this exercise. The s here plays the same role as $$ \dfrac{1}{\lambda}. $$

## a

As we increase s from 0, the training RSS will steadily decrease.

## b

As we increase s from 0, the test RSS decrease initially, and then eventually start increasing in a U shape.

## c

As we increase s from 0, the variance will steadily increase.

## d

As we increase s from 0, the squared bias will steadily decrease.

## e

As we increase s from 0, the irreducible error will remain constant.

# Exercise 4

We can make use of figure 6.5 and 6.8 for this exercise.

## a

As we increase $$ \lambda $$ from 0, the training RSS will steadily increase.

## b

As we increase $$ \lambda $$ from 0, the test RSS decrease initially, and then eventually start increasing in a U shape.

## c

As we increase $$ \lambda $$ from 0, the variance will steadily decrease.

## d

As we increase $$ \lambda $$ from 0, the squared bias will steadily increase.

## e

As we increase $$ \lambda $$ from 0, the irreducible error will remain constant.

# Exercise 5

## a

$$
\left(y_1 - \widehat{\beta_1} x_{11} - \widehat{\beta_2} x_{12} \right)^2 + \left(y_2 - \widehat{\beta_1} x_{21} - \widehat{\beta_2} x_{22} \right)^2 + \lambda \left( \widehat{\beta_1}^2 + \widehat{\beta_2}^2 \right)
$$

## b

Take derivatives with respect to $$ \widehat{\beta_1} $$ and set the equation to zero, we get:

$$
- 2 x_{11} \left(y_1 - \widehat{\beta_1} x_{11} - \widehat{\beta_2} x_{12} \right)
- 2 x_{21} \left(y_2 - \widehat{\beta_1} x_{21} - \widehat{\beta_2} x_{22} \right) 
+ 2 \lambda \widehat{\beta_1} = 0
$$

$$
\lambda \widehat{\beta_1} = 
  x_{11} \left(y_1 - \widehat{\beta_1} x_{11} - \widehat{\beta_2} x_{12} \right)
+ x_{21} \left(y_2 - \widehat{\beta_1} x_{21} - \widehat{\beta_2} x_{22} \right) 
$$

Similarly, Take derivatives with respect to $$ \widehat{\beta_2} $$ and set the equation to zero, we get:

$$
\lambda \widehat{\beta_2} = 
  x_{12} \left(y_1 - \widehat{\beta_1} x_{11} - \widehat{\beta_2} x_{12} \right)
+ x_{22} \left(y_2 - \widehat{\beta_1} x_{21} - \widehat{\beta_2} x_{22} \right)
$$

And since $$ x_{11} = x_{12}, ~ x_{21} = x_{22} $$,

$$
\lambda \widehat{\beta_1} = \lambda \widehat{\beta_2}
$$

Therefore,

$$
\widehat{\beta_1} = \widehat{\beta_2}
$$

## c

$$
\left(y_1 - \widehat{\beta_1} x_{11} - \widehat{\beta_2} x_{12} \right)^2 + \left(y_2 - \widehat{\beta_1} x_{21} - \widehat{\beta_2} x_{22} \right)^2 + \lambda \left( | \widehat{\beta_1} | + | \widehat{\beta_2} | \right)
$$

## d

Doing the same steps as in (b), we obtain

$$
\dfrac{\widehat{\beta_1}}{| \widehat{\beta_1} |} = \dfrac{\widehat{\beta_2}}{| \widehat{\beta_2} |}
$$

$$
\Leftrightarrow \widehat{\beta_1} \widehat{\beta_2} > 0
$$

In other words, the estimated coefficients do not have to be the same value, but they should have the same sign.

# Exercise 6

## a

```{r}
y = 3
lambda = 2
beta = seq(-10, 10, 1)
plot(beta, (y - beta)^2 + lambda * beta^2, pch = 20, xlab = "beta", ylab = "The Ridge")
estimated_beta = y / (1 + lambda)
points(estimated_beta, (y - estimated_beta)^2 + lambda * estimated_beta^2, col = "red", pch = 20)
```

## b

```{r}
y = 3
lambda = 2
beta = seq(-10, 10, 1)
plot(beta, (y - beta)^2 + lambda * beta^2, pch = 20, xlab = "beta", ylab = "The Lasso")
estimated_beta = y / (1 + lambda)
points(estimated_beta, (y - estimated_beta)^2 + lambda * abs(estimated_beta), col = "red", pch = 20)
```

# Exercise 7

## a

$$
\mathcal{L}(Y|X,\beta) = \prod_{i=1}^{n} \frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{\epsilon_i^2}{2\sigma^2}\right) =
\left(\frac{1}{\sqrt{2\pi}\sigma}\right)^n \exp\left(-\frac{1}{2\sigma^2}\sum_{i=1}^n\epsilon_i^2\right)
$$

## b

posterior = likelihood x prior

$$
\mathcal{L}(Y|X,\beta) p(\beta) =
\left(\frac{1}{\sqrt{2\pi}\sigma}\right)^n \exp\left(-\frac{1}{2\sigma^2}\sum_{i=1}^n\epsilon_i^2\right) \left[\frac{1}{2b}\exp\left(-\frac{|\beta|}{b} \right)\right]
$$

## c

Recall from (6.7), the lasso coefficients $$ \beta $$ minimize the quantity

$$
\text{RSS} + \lambda\sum_{j=1}^{p}|\beta_j| 
$$

Mode value is the value the appears the most often or the probability such that value appearing is the largest. In other words, now we need to prove that the problem of maximising the posterior for $$ \beta $$ is the same as the problem of minimising the quantity above. From (b), we have

$$
\mathcal{L}(Y|X,\beta) p(\beta) =
\left(\frac{1}{\sqrt{2\pi}\sigma}\right)^n \exp\left(-\frac{1}{2\sigma^2}\sum_{i=1}^n\epsilon_i^2\right) \left[\frac{1}{2b}\exp\left(-\frac{|\beta|}{b} \right)\right]
$$

$$
log \left( \mathcal{L}(Y|X,\beta) p(\beta) \right) =
\ln\left(\left(\frac{1}{\sqrt{2\pi}\sigma}\right)^n \left(\frac{1}{2b}\right)\right)
 -\left(\frac{1}{2\sigma^2}\sum_{i=1}^n\epsilon_i^2 + \frac{|\beta|}{b}\right)
$$

And in order to maximise the posterior for $$ \beta $$, we need to minimise the last term

$$
\left(\frac{1}{2\sigma^2}\sum_{i=1}^n\epsilon_i^2 + \frac{|\beta|}{b}\right)
$$

$$
= \frac{1}{2\sigma^2}\sum_{i=1}^n\epsilon_i^2 + \frac{1}{b}\sum_{j=1}^{p}|\beta_j|
$$

$$
= \frac{1}{2\sigma^2}\left(\sum_{i=1}^n\epsilon_i^2 + \frac{2\sigma^2}{b}\sum_{j=1}^{p}|\beta_j|\right)
$$

which is equivalent to maximising

$$
\sum_{i=1}^n\epsilon_i^2 + \lambda\sum_{j=1}^{p}|\beta_j|
$$

$$
\text{RSS} + \lambda\sum_{j=1}^{p}|\beta_j| 
$$

Here we have come back to the Lasso optimisation problem.

## d

$$
p(\beta) = \prod_{i=1}^p p(\beta_i) = \prod_{i=1}^p \frac{1}{\sqrt{2\pi c}} \exp\left(-\frac{\beta_i^2}{2c}\right) = \left( \frac{1}{\sqrt{2\pi c}} \right)^p \exp\left(-\frac{1}{2c} \sum_{i=1}^p \beta_i^2\right)
$$

So, the posterior for $$ \beta $$ can be written as

$$
\mathcal{L}(Y|X,\beta) p(\beta) =
\left(\frac{1}{\sqrt{2\pi}\sigma}\right)^n \exp\left(-\frac{1}{2\sigma^2}\sum_{i=1}^n\epsilon_i^2\right) \left( \frac{1}{\sqrt{2\pi c}} \right)^p \exp\left(-\frac{1}{2c} \sum_{i=1}^p \beta_i^2\right)
$$

$$
= \left(\frac{1}{\sqrt{2\pi}\sigma}\right)^n \left( \frac{1}{\sqrt{2\pi c}} \right)^p \exp\left(-\frac{1}{2\sigma^2}\sum_{i=1}^n\epsilon_i^2 -\frac{1}{2c}\sum_{i=1}^p \beta_i^2\right)
$$

## e

Using the same idea from (c), we would like to maximise

$$
\mathcal{L}(Y|X,\beta) p(\beta) = \left(\frac{1}{\sqrt{2\pi}\sigma}\right)^n \left( \frac{1}{\sqrt{2\pi c}} \right)^p \exp\left(-\frac{1}{2\sigma^2}\sum_{i=1}^n\epsilon_i^2 -\frac{1}{2c}\sum_{i=1}^p \beta_i^2\right)
$$

which is equivalent to maximising

$$
\ln\left(\left(\frac{1}{\sqrt{2\pi}\sigma}\right)^n \left( \frac{1}{\sqrt{2\pi c}} \right)^p \right) 
-\left(\frac{1}{2\sigma^2}\sum_{i=1}^n\epsilon_i^2 + \frac{1}{2c}\sum_{i=1}^p \beta_i^2\right)
$$

and minimising the last term

$$
\frac{1}{2\sigma^2}\sum_{i=1}^n\epsilon_i^2 + \frac{1}{2c}\sum_{i=1}^p \beta_i^2
$$

$$
minimise _\beta \left(\frac{1}{2\sigma^2}\sum_{i=1}^n\epsilon_i^2 + \frac{1}{2c}\sum_{i=1}^p \beta_i^2\right)
$$

$$
= minimise_\beta \frac{1}{2\sigma^2} \left(\sum_{i=1}^n\epsilon_i^2 + \frac{2\sigma^2}{2c}\sum_{i=1}^p \beta_i^2\right) 
$$

$$
= minimise_\beta\left(\sum_{i=1}^n\epsilon_i^2 + \lambda\sum_{i=1}^p \beta_i^2\right)
=minimise_\beta \left(\text{RSS} + \lambda\sum_{i=1}^p \beta_i^2\right)
$$

Here we have come back to the Ridge optimisation problem.

# Exercise 8

## a

```{r}
set.seed(1)
X = rnorm(100)
noise = rnorm(100)
```

## b

```{r}
Y = 1 + 2*X + 3*X^2 + 4*X^3 + noise 
```

```{r}
plot(X, Y)
```

## c

```{r}
library(leaps)
```

```{r}
data = data.frame(Y, X)
regfit_full = regsubsets(Y ~ poly(X, 10), data = data)
regfit_summary = summary(regfit_full)
regfit_summary
```

```{r}
par(mfrow = c(2, 2))

plot(regfit_summary$cp, xlab = 'Number of variables', ylab = 'Cp', type = 'l')
cp_min = which.min(regfit_summary$cp)
points(cp_min, regfit_summary$cp[cp_min], col= 'red', cex = 2, pch = 20)

plot(regfit_summary$bic, xlab = 'Number of variables', ylab = 'BIC', type = 'l')
bic_min = which.min(regfit_summary$bic)
points(bic_min, regfit_summary$bic[bic_min], col= 'red', cex = 2, pch = 20)

plot(regfit_summary$adjr2, xlab = 'Number of variables', ylab = 'AdjR2', type = 'l')
adjr2_max = which.max(regfit_summary$adjr2)
points(adjr2_max, regfit_summary$adjr2[adjr2_max], col= 'red', cex = 2, pch = 20)
```

```{r}
coef(regfit_full, 3)
coef(regfit_full, 4)
coef(regfit_full, 5)
```

## d

The result obtained from forward stepwise selection is similar to the result obtained from (c)

```{r}
regfit_fwd = regsubsets(Y ~ poly(X, 10), data = data,
                        nvmax = 10, method = 'forward')
coef(regfit_fwd, 3)
coef(regfit_fwd, 4)
coef(regfit_fwd, 5)
```

And so is backward stepwise selection.

```{r}
regfit_bwd = regsubsets(Y ~ poly(X, 10), data = data,
                        nvmax = 10, method = 'backward')
coef(regfit_bwd, 3)
coef(regfit_bwd, 4)
coef(regfit_bwd, 5)
```

## e

```{r}
library(glmnet)
```

```{r}
set.seed(1)

grid = 10^seq(10, -2, length = 100)
cv_lasso = cv.glmnet(poly(X, 10), Y, alpha = 1, lambda = grid)
plot(cv_lasso)
```

The Lasso result suggests that there 6 significant predictors for Y. The most significant predictors, however, still $$ X, ~X^2, ~ X^3 $$ and its coefficients are still virtually the same as in (c)

```{r}
best_lambda = cv_lasso$lambda.min
coef(glmnet(poly(X, 10), Y, alpha = 1, lambda = best_lambda))
```

## f

### Subset selection

```{r}
Y = 1 + 2*X^7 + noise
plot(X, Y)
```

```{r}
regfit_full = regsubsets(Y ~ poly(X, 10), data = data.frame(X, Y))
summary(regfit_full)
```

```{r}
coef(regfit_full, 1)
coef(regfit_full, 3)
```

### Lasso

```{r}
set.seed(1)

grid = 10^seq(10, -2, length = 100)
cv_lasso = cv.glmnet(poly(X, 10), Y, alpha = 1, lambda = grid)
plot(cv_lasso)
```

```{r}
best_lambda = cv_lasso$lambda.min
coef(glmnet(poly(X, 10), Y, alpha = 1, lambda = best_lambda))
```

```{r}
coefficients = coef(glmnet(poly(X, 10), Y, alpha = 1, lambda = best_lambda))
Y_pred = poly(X, 10) %*% coefficients[2:11] + coefficients[1] 
plot(X, Y, col = 'red')
points(X, Y_pred)
legend(legend = c('Y_true', 'Y_pred'), 
       col = c('red', 'black'), 
       x = 'topleft', lty = c(0, 0), pch = c(2, 2))
```

The subset selection and Lasso approach results agree with each other. However, both models overfit the data.

# Exercise 9

```{r}
library(ISLR2)
library(dplyr)
college = College %>% relocate(Apps, .before = Private)
head(college)
row.names(college) = NULL
```

## a

```{r}
set.seed(1)
train_indices = sample(777, 666)
train_data = college[train_indices, ]
test_data = college[-train_indices, ]
Y_test_true = test_data$Apps
```

## b

```{r}
lsr_model = lm(Apps ~ ., data = train_data)
Y_lsr_pred = predict(lsr_model, newdata = test_data)
lsr_rmse = sqrt(mean((Y_lsr_pred - Y_test_true)^2))
lsr_rmse
```

## c

```{r}
set.seed(1)
lambdas = 10^seq(10, -3, length = 100)
ridge_cv = cv.glmnet(model.matrix(Apps ~ ., data = train_data)[, -1],
                   train_data[, 1], lambda = lambdas, alpha = 0)
best_ridge_lambda = ridge_cv$lambda.min
plot(ridge_cv)
```

```{r}
ridge_model = glmnet(model.matrix(Apps ~ ., data = train_data)[, -1],
                     train_data[, 1], lambda = best_ridge_lambda, alpha = 0)

Y_ridge_pred = predict(ridge_model, s = best_ridge_lambda, 
                       newx = model.matrix(Apps ~ ., data = test_data)[, -1]) 

ridge_rmse = sqrt(mean(Y_ridge_pred - Y_test_true)^2)
ridge_rmse
```

```{r}
coef(ridge_model)
```

## d

```{r}
set.seed(1)
lambdas = 10^seq(10, -3, length = 100)
lasso_cv = cv.glmnet(model.matrix(Apps ~ ., data = train_data)[, -1],
                     train_data[, 1], lambda = lambdas, alpha = 1)
best_lasso_lambda = lasso_cv$lambda.min
plot(lasso_cv)
```

```{r}
lasso_model = glmnet(model.matrix(Apps ~ ., data = train_data)[, -1],
                     train_data[, 1], lambda = best_lasso_lambda, alpha = 1)

Y_lasso_pred = predict(lasso_model, s = best_lambda, 
                       newx = model.matrix(Apps ~ ., data = test_data)[, -1]) 

lasso_rmse = sqrt(mean(Y_lasso_pred - Y_test_true)^2)
lasso_rmse
```

Just another way to calculate predicted values

```{r}
Y_lasso_pred = model.matrix(Apps ~ ., data = test_data)[, -1] %*% coef(lasso_model)[2:18] + coef(lasso_model)[1]

lasso_rmse = sqrt(mean(Y_lasso_pred - Y_test_true)^2)
lasso_rmse
```

```{r}
coef(lasso_model)
```

## e

```{r}
library(pls)
set.seed(1)
pcr_model = pcr(Apps ~ ., data = train_data, scale = T, validation = 'CV')
validationplot(pcr_model)
```

PCR is more difficult to interpret since it doesn't preform variable selection nor produce coefficient estimates.

```{r}
pcr_model = pcr(Apps ~ ., data = train_data, scale = T, ncomp = 17)
Y_pcr_pred = predict(pcr_model, newdata = test_data[, 2:18])
pcr_rmse = sqrt(mean(Y_pcr_pred - Y_test_true)^2)
pcr_rmse
```

## f

```{r}
set.seed(1)
plsr_model = plsr(Apps ~ ., data = train_data, scale = T, validation = 'CV')
validationplot(plsr_model)
```

```{r}
summary(plsr_model)
```

```{r}
plsr_model = plsr(Apps ~ ., data = train_data, scale = T, ncomp = 13)
Y_plsr_pred = predict(plsr_model, newdata = test_data[, 2:18])
plsr_rmse = sqrt(mean(Y_plsr_pred - Y_test_true)^2)
plsr_rmse
```

## g

We can predict the number of college applications received pretty accurately using Ridge Regression, Lasso, PCR or Partial least square approach. The Ridge and Lasso models have the same results and are interpretable. The PCR and PLS methods provide different results and are difficult to interpret.

# Exercise 10

## a

```{r}
set.seed(1)
X = matrix(rnorm(1000 * 20), 1000, 20)
e = rnorm(1000)
B = rnorm(20) + 1

for (i in c(1,3,5,7,9)){
  B[i] = 0
}

Y = X %*% B + e
```

## b

```{r}
train_data = data.frame(Y, X)[1:100, ]
test_data = data.frame(Y, X)[101: 1000, ]
head(train_data)
```

## c

```{r}
predict_regsubsets = function ( object , newdata , id , ...) {
  form = as.formula ( object $ call [[2]])
  mat = model.matrix ( form , newdata )
  coefi = coef ( object , id = id )
  xvars = names ( coefi )
  mat [ , xvars ] %*% coefi
}
```

```{r}
ss_model = regsubsets(Y ~ ., data = train_data, nvmax = 20)

train_mse = rep(NA, 20)
for (i in 1:20){
  Y_train_pred = predict_regsubsets(ss_model, train_data, id = i)
  train_mse[i] = mean((Y_train_pred - train_data$Y)^2)
}
```

```{r}
plot(1:20, train_mse)
which.min(train_mse)
```

## d

```{r}
test_mse = rep(NA, 20)
for (i in 1:20){
  Y_test_pred = predict_regsubsets(ss_model, test_data, id = i)
  test_mse[i] = mean((Y_test_pred - test_data$Y)^2)
}

dim(Y_test_pred)
```

```{r}
plot(1:20, test_mse)
which.min(test_mse)
```

## e

The test set MSE takes on its minimum value for the 17-variable model. Specifically, it excludes the first, the seventh and ninth variable (recall that the true values for variable number 1, 3, 5, 7, 9 are zero).

```{r}
coef(ss_model, id = 17)
```

## f

The mean square of the difference in coefficients is 0.097 and its standard deviation is 0.047.

```{r}
best_coef = coef(ss_model, id = 17)
best_variables = rownames(data.frame(best_coef))
difference = rep(NA, 17)
for (i in 2:length(best_variables)){
  num = as.numeric(gsub('X', '', best_variables[i]))
  sqr = sqrt((B[num] - best_coef[i])^2)
  difference[i-1] = sqr
}

mean(B)
mean(difference)
sd(difference)
```

```{r}
x = seq(min(B), max(B), length = 100) * sd(difference) + mean(difference)
y = dnorm(x, mean(difference), sd(difference))
plot(x, y)
```

Here we can have a look at how accurate the prediction is from the first 90 test observations. It is not too surprising or disapointing.

```{r}
Y_test_pred = predict_regsubsets(ss_model, test_data, id = 17)
plot(1:90, test_data$Y[1:90])
points(1:90, Y_test_pred[1:90], col = 'red')
```

## g

```{r}
SQR = rep(NA, 20)

for (r in 1:20){
  variable_names = rownames(data.frame(coef(ss_model, id = r)))
  coefficients = coef(ss_model, id = r)
  sqr = 0
  for (i in 2:length(variable_names)){
    num = as.numeric(gsub('X', '', variable_names[i]))
    sqr = sqr + sqrt((B[num] - coefficients[i])^2)
  }
  SQR[r] = sqr
}
```

```{r}
plot(1:20, SQR, type = 'l')
```

The plot displaying the difference in coefficients is pretty different from the test MSE plot. The more variables used, the greater the difference. This makes sense since it is more likely to get closer to the true coefficients if we use less variables. However, we should pay more attention to the response values.

# Exercise 11

```{r}
head(Boston)
dim(Boston)
```

```{r}
set.seed(1)

shuffled_data = Boston[sample(1:506, ), ]
train_data = shuffled_data[1:400, ]
test_data = shuffled_data[401:506, ]
```

## a

### Subset selection

```{r}
set.seed(1)

k = 10
n = nrow(train_data)
folds = sample(rep(1:k, length = n))
cv_errors = matrix(NA, k, 12, dimnames = list(NULL, paste(1:12)))
```

```{r}
for (j in 1:k){
  
  best_fit = regsubsets(crim ~., data = train_data[folds != j, ], nvmax = 12)
  for (i in 1:12){
    pred = predict_regsubsets(best_fit, train_data[folds == j, ], id = i)
    cv_errors[j, i] = mean((train_data$crim[folds == j] - pred)^2)
  }
}
```

```{r}
mean_cv_errors = apply(cv_errors, 2 ,mean)
mean_cv_errors
plot(mean_cv_errors, type = 'b')
```

```{r}
best_ss_model = regsubsets(crim ~., data = train_data, nvmax = 12)
coef(best_ss_model, 7)
```

```{r}
ss_test_pred = predict_regsubsets(best_ss_model, test_data, id = 7)
ss_test_rmse = sqrt(mean(ss_test_pred - test_data$crim)^2)
ss_test_rmse
```

### Lasso

```{r}
set.seed(1)
lambdas = 10^seq(10, -3, length = 100)
lasso_cv = cv.glmnet(model.matrix(crim ~ ., data = train_data)[, -1],
                     train_data[, 1], lambda = lambdas, alpha = 1)
best_lasso_lambda = lasso_cv$lambda.min
plot(lasso_cv)
```

```{r}
lasso_model = glmnet(model.matrix(crim ~ ., data = train_data)[, -1],
                     train_data[, 1], lambda = best_lasso_lambda, alpha = 1)

crim_lasso_pred = predict(lasso_model, s = best_lasso_lambda, 
                       newx = model.matrix(crim ~ ., data = test_data)[, -1]) 

lasso_test_rmse = sqrt(mean(crim_lasso_pred - test_data$crim)^2)
lasso_test_rmse
```

### PCR

```{r}
set.seed(1)
pcr_model = pcr(crim ~ ., data = train_data, scale = T, validation = 'CV')
validationplot(pcr_model)
```

```{r}
pcr_model = pcr(crim ~ ., data = train_data, scale = T, ncomp = 12)
crim_pcr_pred = predict(pcr_model, newdata = test_data)
pcr_rmse = sqrt(mean(crim_pcr_pred - test_data$crim)^2)
pcr_rmse
```

### Partial Least Square

```{r}
set.seed(1)
plsr_model = plsr(crim ~ ., data = train_data, scale = T, validation = 'CV')
validationplot(plsr_model)
data.frame(plsr_model$validation$adj)
```

```{r}
plsr_model = plsr(crim ~ ., data = train_data, scale = T, ncomp = 10)
crim_plsr_pred = predict(plsr_model, newdata = test_data)
plsr_rmse = sqrt(mean(crim_plsr_pred - test_data$crim)^2)
plsr_rmse
```

## b

Among the four approaches from (a), we could choose the Lasso as the best model since it gives us the lowest test error and it is also more interpretable, relative to PCR and PLS.

## c

```{r}
coef(lasso_model)
```

Using all variables would lead to overfitting. The Lasso model only uses 10 over 12 variables.
