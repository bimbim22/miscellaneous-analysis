---
title: "Chapter 2 Statistical Learning"
output:
  pdf_document: default
---

```{r}
library(ISLR)
library(ISLR2)
library(psych)
library(ggplot2)
```

# Exercise 1

## a

The performance of a flexible statistical learning method can be better since we might need underlying predictors.

## b

An inflexible statistical learning method because the risk of overfitting has already been high, a flexible statistical learning method may exaggerate it

## c

A flexible statistical learning method could be better as it provides more flexibility.

## d

An inflexible statistical learning method since a flexible model would also fit the noise of the error term.

# Exercise 2

## a

This is a regression problem and we are interested in inference. Here n = 500 and p = 3.

## b

This is a classification problem and we are interested in prediction. Here n = 20 and p = 13.

## c

This is a regression problem and we are interested in prediction. Here n = 52 and p = 3.

# Exercise 3

## a

![](images/chap12ex2.PNG)

## b

More flexible statistical methods have higher variance because small changes in the training data can result in large changes in $$ \hat{f} $$. On the other hand, bias refers to the error that is introduced by approximating a real-life problem. Generally, more flexible methods result in less bias.

In terms of training MSE and test MSE, as flexibility increases, the model fits training observations more closely, the training MSE therefore decreases. Test MSE, however, only goes down to a certain point then bounces off since the model poorly generalises on the test set.

And the Bayes curve (the irreducible error) is constant and smaller the the test error.

# Exercise 4

## a

1/ We want to predict if a data scientist's salary (response) is under or above the average based factors like education level, years of experience or age (predictors) . The goal is prediction.

2/ Traders want to predict whether a particular stock price will increase tomorrow (response) based on prices from previous days (predictors). The goal is prediction.

3/ Researchers want to know which indicators, gender, age at sexual debut or ethnicity (predictors) are highly associated with patients who exposured to HIV (response). The goal is inference.

## b

1/ We want to know which factors like education level, years of experience or age (predictors) strongly affect a data scientist's salary (response). The goal is inference.

2/ Traders want to predict a particular stock price tomorrow (response) based on prices from previous days (predictors). The goal is prediction.

3/ Researchers want to predict the probability (response) that a patient has been exposured to HIV from three indicators: gender, age at sexual debut or ethnicity. The goal is prediction.

# Exercise 5

A very flexible model might be able to figure out the underlying pattern (when p is small) and be better when the true relationship between the predictors and the response is non-linear. On the other hand, due to its complexity, we may have to sacrifice interpretability and may run the risk of overfitting (A less flexible approach is preferred in general when the goal is inference). Please revise exercise 2 for under which circumstances we prefer a more flexible approach or less flexible approach.

# Exercise 6

Non-parametric methods do not make explicit assumptions about the functional form of f. Instead they seek an estimate of f that gets as close to the data points as possible without being too rough or wiggly. Such approaches can have a major advantage over parametric approaches: by avoiding the assumption of a particular functional form for f, they have the potential to accurately fit a wider range of possible shapes for f. Any parametric approach brings with it the possibility that the functional form used to estimate f is very different from the true f, in which case the resulting model will not fit the data well. In contrast, non-parametric approaches completely avoid this danger, since essentially no assumption about the form of f is made. But non-parametric approaches do suffer from a major disadvantage: since they do not reduce the problem of estimating f to a small number of parameters, a very large number of observations (far more than is typically needed for a parametric approach) is required in order to obtain an accurate estimate for f.

# Exercise 7

```{r}
data = c(0, 2, 0, 0, -1, 1, 0, 3, 0, 1, 1, 0, 1, 0, 0, 0, 3, 2, 1, 1, 0)
data = matrix(data, nrow = 7, ncol = 3)
data
```

## a

```{r}
dist(data, method = 'euclidean')
```

## b

Because the single nearest neighbour (observation number 5), is green so our prediction is green.

## c

Among the three nearest neighbours (observations 5, 6 and 2), two of them are red so our prediction will be red. Or mathematically speaking,

$$
P(Y = \mathrm{Red} | X = x_0) = \frac{1}{K}\sum_{i\in\mathcal{N}_0}I(y_i = \mathrm{Red}) = \frac{1}{3}(1 + 0 + 1) = \frac{2}{3}
$$

$$ P(Y = \mathrm{Green} | X = x_0) = \frac{1}{K}\sum_{i\in\mathcal{N}_0}I(y_i = \mathrm{Green}) = \frac{1}{3}(0 + 1 + 0) = \frac{1}{3}$$

## d

When the relationship is non-linear, we need a more flexible classification method, a larger 1/K which equivalent to a smaller K.

# Exercise 8

## a

```{r}
head(College)
```

## b

```{r}
View(College)
```

## c

```{r}
pairs(College[,1:10])
```

```{r}
plot(College$Private, College$Outstate)
```

```{r}
Elite = rep('No', nrow(College))
Elite[College$Top10perc > 50] = 'Yes'
Elite = as.factor(Elite)
College = data.frame(College, Elite)
summary(College$Elite)
View(College)

plot( College$Elite, College$Outstate)
```

```{r}
par(mfrow = c(2, 2))
hist(College$Books, col = 2, xlab = 'Books', ylab = 'Count')
hist(College$Personal, col = 3, xlab = 'Personal', ylab = 'Quantity')
hist(College$Expend, col = 4, xlab = 'Expend', ylab = 'Quantity')
hist(College$Apps, col = 5, xlab = 'Apps', ylab = 'Quantity')
```

# Exercise 9

## a

```{r}
head(Auto)
auto = na.omit(Auto)
summary(auto$cylinders)
# cylinders, year, origin and name are qualitative predictors
```

## b

```{r}
qualitative_indices = match(c('cylinders', 'year', 'origin', 'name'), 
                            colnames(auto))
sapply(auto[, -qualitative_indices], range)
```

## c

```{r}
sapply(auto[, -qualitative_indices], mean)
sapply(auto[, -qualitative_indices], sd)
```

## d

```{r}
mini_auto = auto[-c(10:85), ]
sapply(mini_auto[, -qualitative_indices], range)
sapply(mini_auto[, -qualitative_indices], mean)
sapply(mini_auto[, -qualitative_indices], sd)
```

## e

```{r}
pairs.panels(auto)
```

## f

Variables like cylinders, displacement, horsepower, weight are linearly related with the target mpg (the correlation coefficients are greater than 0.7). Hence, these predictors might be useful for prediction.

# Exercise 10

## a

```{r}
head(Boston)
```

```{r}
?Boston
```

## b

```{r}
pairs.panels(Boston)
```

## c

```{r}
hist(Boston$crim)
```

Variables are somewhat linearly related with the target crime rate (the correlation coefficients are around 0.4). Hence, these predictors might be useful for prediction.

## d

```{r}
par(mfrow = c(2,2))
hist(Boston$crim, col = 2)
hist(Boston$tax, col = 3)
hist(Boston$ptratio, col = 4)
```

## e

```{r}
nrow(Boston[Boston$chas == 1, ])
```

## f

```{r}
median(Boston$ptratio)
```

## g

```{r}
which.min(Boston$medv)
Boston[399, ]
```

## h

```{r}
nrow(Boston[Boston$rm > 7, ])
nrow(Boston[Boston$rm > 8, ])
```
